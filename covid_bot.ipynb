{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTZ5CPSyQIbi7y+Ksa8RPp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuragdotexe/covid-bot/blob/main/covid_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "5jXUOGtyzcVY",
        "outputId": "be571303-cc3e-4da2-ada9-0cc79f78e3fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\my files\\\\Data Analysis\\\\covid bot\\\\WHO .json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c11cd2d3ca60>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#loading the json data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C:\\my files\\Data Analysis\\covid bot\\WHO .json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\my files\\\\Data Analysis\\\\covid bot\\\\WHO .json'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy\n",
        "#import tflearn\n",
        "import tensorflow\n",
        "import pickle\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "#loading the json data\n",
        "with open(\"C:\\my files\\Data Analysis\\covid bot\\WHO .json\") as file:\n",
        "\tdata = json.load(file)\n",
        "\n",
        "#print(data[\"intents\"])\n",
        "try:\n",
        "\twith open(\"data.pickle\", \"rb\") as f:\n",
        "\t\twords, l, training, output = pickle.load(f)\n",
        "except:\n",
        "\n",
        "\t# Extracting Data\n",
        "\twords = []\n",
        "\tl = []\n",
        "\tdocs_x = []\n",
        "\tdocs_y = []\n",
        "\n",
        "# converting each pattern into list of words using nltk.word_tokenizer\n",
        "\tfor i in data[\"intents\"]:\n",
        "\t\tfor p in i[\"patterns\"]:\n",
        "\t\t\twrds = nltk.word_tokenize(p)\n",
        "\t\t\twords.extend(wrds)\n",
        "\t\t\tdocs_x.append(wrds)\n",
        "\t\t\tdocs_y.append(i[\"tag\"])\n",
        "\n",
        "\t\t\tif i[\"tag\"] not in l:\n",
        "\t\t\t\tl.append(i[\"tag\"])\n",
        "\t# Word Stemming\n",
        "\twords = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "\twords = sorted(list(set(words)))\n",
        "\tl = sorted(l)\n",
        "\n",
        "\t# This code will simply create a unique list of stemmed\n",
        "\t# words to use in the next step of our data preprocessing\n",
        "\ttraining = []\n",
        "\toutput = []\n",
        "\tout_empty = [0 for _ in range(len(l))]\n",
        "\tfor x, doc in enumerate(docs_x):\n",
        "\t\tbag = []\n",
        "\n",
        "\t\twrds = [stemmer.stem(w) for w in doc]\n",
        "\n",
        "\t\tfor w in words:\n",
        "\t\t\tif w in wrds:\n",
        "\t\t\t\tbag.append(1)\n",
        "\t\t\telse:\n",
        "\t\t\t\tbag.append(0)\n",
        "\t\toutput_row = out_empty[:]\n",
        "\t\toutput_row[l.index(docs_y[x])] = 1\n",
        "\n",
        "\t\ttraining.append(bag)\n",
        "\t\toutput.append(output_row)\n",
        "\n",
        "\t# Finally we will convert our training data and output to numpy arrays\n",
        "\ttraining = numpy.array(training)\n",
        "\toutput = numpy.array(output)\n",
        "\twith open(\"data.pickle\", \"wb\") as f:\n",
        "\t\tpickle.dump((words, l, training, output), f)\n",
        "\n",
        "\n",
        "# Developing a Model\n",
        "tensorflow.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "\n",
        "# remove comment to not train model after you satisfied with the accuracy\n",
        "model = tflearn.DNN(net)\n",
        "\"\"\"try:\n",
        "\tmodel.load(\"model.tflearn\")\n",
        "except:\"\"\"\n",
        "\n",
        "# Training & Saving the Model\n",
        "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save(\"model.tflearn\")\n",
        "\n",
        "# making predictions\n",
        "def bag_of_words(s, words):\n",
        "\tbag = [0 for _ in range(len(words))]\n",
        "\n",
        "\ts_words = nltk.word_tokenize(s)\n",
        "\ts_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "\tfor se in s_words:\n",
        "\t\tfor i, w in enumerate(words):\n",
        "\t\t\tif w == se:\n",
        "\t\t\t\tbag[i] = 1\n",
        "\n",
        "\treturn numpy.array(bag)\n",
        "\n",
        "\n",
        "def chat():\n",
        "\tprint(\"\"\"Start talking with the bot and ask your\n",
        "\tqueries about Corona-virus(type quit to stop)!\"\"\")\n",
        "\n",
        "\twhile True:\n",
        "\t\tinp = input(\"You: \")\n",
        "\t\tif inp.lower() == \"quit\":\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\tresults = model.predict([bag_of_words(inp, words)])[0]\n",
        "\t\tresults_index = numpy.argmax(results)\n",
        "\n",
        "\t\t#print(results_index)\n",
        "\t\ttag = l[results_index]\n",
        "\t\tif results[results_index] > 0.7:\n",
        "\t\t\tfor tg in data[\"intents\"]:\n",
        "\t\t\t\tif tg['tag'] == tag:\n",
        "\t\t\t\t\tresponses = tg['responses']\n",
        "\n",
        "\t\t\tprint(random.choice(responses))\n",
        "\t\telse:\n",
        "\t\t\tprint(\"I am sorry but I can't understand\")\n",
        "\n",
        "chat()\n"
      ]
    }
  ]
}